{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File to test things on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 1], [1, 1]], (2, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [np.array([[1,1],[1,1]]), [1,1]]\n",
    "\n",
    "p = list(map(list,a[0]))\n",
    "c = np.array(a[1])\n",
    "\n",
    "p, a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2], 4, [4, 4], [1, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_trajectories(self, env):\n",
    "        \"\"\"\n",
    "        Given a Unity Environment and a Q-Network, this method will generate a\n",
    "        buffer of Experiences obtained by running the Environment with the Policy\n",
    "        derived from the Q-Network.\n",
    "        :param BaseEnv: The UnityEnvironment used.\n",
    "        :param q_net: The Q-Network used to collect the data.\n",
    "        :param buffer_size: The minimum size of the buffer this method will return.\n",
    "        :param epsilon: Will add a random normal variable with standard deviation.\n",
    "        epsilon to the value heads of the Q-Network to encourage exploration.\n",
    "        :returns: a Tuple containing the created buffer and the average cumulative\n",
    "        the Agents obtained.\n",
    "        \"\"\"\n",
    "        \n",
    "        buffer = []\n",
    "\n",
    "        # Read and store the Behavior Name of the Environment\n",
    "        behavior_name = list(env.behavior_specs)[0]\n",
    "        # Read and store the Behavior Specs of the Environment\n",
    "        spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "        # Create a Mapping from AgentId to Trajectories. This will help us create\n",
    "        # trajectories for each Agents\n",
    "        dict_trajectories_from_agent = {}\n",
    "        # Create a Mapping from AgentId to the last observation of the Agent\n",
    "        dict_last_obs_from_agent = {}\n",
    "        # Create a Mapping from AgentId to the last observation of the Agent\n",
    "        dict_last_action_from_agent = {}\n",
    "        # Create a Mapping from AgentId to cumulative reward (Only for reporting)\n",
    "        dict_cumulative_reward_from_agent = {}\n",
    "        # Create a list to store the cumulative rewards obtained so far\n",
    "        cumulative_rewards = []\n",
    "\n",
    "        while len(buffer) < self.buffer_size:  # While not enough data in the buffer\n",
    "            # Get the Decision Steps and Terminal Steps of the Agents\n",
    "            decision_steps, terminal_steps =env.get_steps(behavior_name)\n",
    "\n",
    "            # permute the tensor to go from NHWC to NCHW\n",
    "            # order = (0, 3, 1, 2)\n",
    "            # decision_steps.obs = [np.transpose(obs, order) for obs in decision_steps.obs]\n",
    "            # terminal_steps.obs = [np.transpose(obs, order) for obs in terminal_steps.obs]\n",
    "\n",
    "            # For all Agents with a Terminal Step:\n",
    "            for agent_id_terminated in terminal_steps:\n",
    "                # Create its last experience (is last because the Agent terminated)\n",
    "                \n",
    "                last_experience = Experience(\n",
    "                                            obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "                                            reward=terminal_steps[agent_id_terminated].reward,\n",
    "                                            done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "                                            action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "                                            next_obs=terminal_steps[agent_id_terminated].obs[0]\n",
    "                                            )\n",
    "                # Clear its last observation and action (Since the trajectory is over)\n",
    "                dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "                dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "\n",
    "                # Report the cumulative reward\n",
    "                cumulative_reward = (dict_cumulative_reward_from_agent.pop(agent_id_terminated)+ terminal_steps[agent_id_terminated].reward\n",
    "                )\n",
    "                cumulative_rewards.append(cumulative_reward)\n",
    "                # Add the Trajectory and the last experience to the buffer\n",
    "                buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "                buffer.append(last_experience)\n",
    "                \n",
    "\n",
    "            # For all Agents with a Decision Step:\n",
    "            for agent_id_decisions in decision_steps:\n",
    "                # If the Agent does not have a Trajectory, create an empty one\n",
    "                if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "                    dict_trajectories_from_agent[agent_id_decisions] = []\n",
    "                    dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
    "\n",
    "                # If the Agent requesting a decision has a \"last observation\"\n",
    "                if agent_id_decisions in dict_last_obs_from_agent:\n",
    "                # Create an Experience from the last observation and the Decision Step\n",
    "                    exp = Experience(\n",
    "                        obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
    "                        reward=decision_steps[agent_id_decisions].reward,\n",
    "                        done=False,\n",
    "                        action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
    "                        next_obs=decision_steps[agent_id_decisions].obs[0],\n",
    "                    )\n",
    "                    # Update the Trajectory of the Agent and its cumulative reward\n",
    "                    dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
    "                    dict_cumulative_reward_from_agent[agent_id_decisions] += (\n",
    "                        decision_steps[agent_id_decisions].reward\n",
    "                    )\n",
    "                # Store the observation as the new \"last observation\"\n",
    "                dict_last_obs_from_agent[agent_id_decisions] = (\n",
    "                decision_steps[agent_id_decisions].obs[0]\n",
    "                )\n",
    "\n",
    "            # Generate an action for all the Agents that requested a decision\n",
    "            # Compute the values for each action given the observation\n",
    "            actions = (\n",
    "                self.network(torch.from_numpy(decision_steps.obs[0])).detach().numpy()\n",
    "            )\n",
    "\n",
    "            # Add some noise with epsilon to the values\n",
    "            # actions_values += epsilon * (\n",
    "            #     np.random.randn(actions_values.shape[0], actions_values.shape[1])\n",
    "            # ).astype(np.float32)\n",
    "\n",
    "            # Pick the best action using argmax\n",
    "            # actions = np.argmax(actions_values, axis=1)\n",
    "            # print(actions, actions.shape)\n",
    "            # actions.resize((len(decision_steps), 1))\n",
    "            \n",
    "            # Store the action that was picked, it will be put in the trajectory later\n",
    "            for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "                dict_last_action_from_agent[agent_id] = actions\n",
    "\n",
    "            # Set the actions in the environment\n",
    "            # Unity Environments expect ActionTuple instances.\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_continuous(actions)\n",
    "            env.set_actions(behavior_name, action_tuple)\n",
    "            # Perform a step in the simulation\n",
    "            env.step()\n",
    "        return buffer, np.mean(cumulative_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('DLGSproject': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04aa4936bd3c5ea971897fa7a9eb858c1bf545cdb5b656a168af9d1fbe9e0a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
